{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрическая классификация. Параграф №3 и №5. [Видео](https://www.youtube.com/watch?v=GyOxB2itxnc)\n",
    "1. Гипотезы компактности и непрерывности.\n",
    "2. Обобщённый метрический классификатор.\n",
    "3. Метод ближайших соседей kNN и его обобщения. Подбор числа k по критерию скользящего контроля.\n",
    "4. Метод окна Парзена с постоянной и переменной шириной окна.\n",
    "5. Метод потенциальных функций и его связь с линейной моделью классификации.\n",
    "6. Задача отбора эталонов. Полный скользящий контроль (CCV), формула быстрого вычисления для метода 1NN. Профиль компактности (46:55 в видео).\n",
    "7. Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля.\n",
    "8. Непараметрическая регрессия. Локально взвешенный метод наименьших квадратов. Ядерное сглаживание. (п.5.2)\n",
    "9. Оценка Надарая-Ватсона с постоянной и переменной шириной окна. Выбор функции ядра (1:14:35 в видео) и ширины окна сглаживания. (п.5.2) \n",
    "10. Задача отсева выбросов. Робастная непараметрическая регрессия. Алгоритм LOWESS. (п.5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные методы классификации. Логистическая регрессия. Параграф №4. [Видео](https://www.youtube.com/watch?v=thrPR77K-os&feature=youtu.be)\n",
    "1. Линейный классификатор, модель МакКаллока-Питтса, непрерывные аппроксимации пороговой функции потерь.\n",
    "    * Понятие отступа для разделяющего классификатора (12:10). Обобщение.\n",
    "    * Часто используемые непрервыные функции потерь (16:25)\n",
    "    * Модель МакКаллока-Питтса (18:12)\n",
    "2. Метод стохастического градиента SG. Параграф 4.3\n",
    "    * Градиентный метод численной минимизации (23:05)\n",
    "    * Экспоненциальное скользящее среднее (31:00). Рекуррентная оценка функционала    \n",
    "3. Метод стохастического среднего градиента SAG. (32:20)\n",
    "    * Метод накопления импульса (momentum) (35:00)    \n",
    "4. Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, «выбивание» из локальных минимумов.\n",
    "    * Варианты инициализации весов (37:45)\n",
    "    * Порядок предъявления объектов (42:35)\n",
    "    * Выбор градиентного шага (44:25). Метод Ньютона-Рафсона\n",
    "5. Проблема мультиколлинеарности и переобучения, регуляризация или редукция весов (weight decay). (50:50)\n",
    "    * Повторить что такое линейная зависимость.\n",
    "    * Что такое мультиколлинеарность формально (52:45)\n",
    "    * Регуляризация (сокращение весов) (56:23)\n",
    "6. Вероятностная постановка задачи классификации. Принцип максимума правдоподобия. (1:00:15)\n",
    "7. Вероятностная интерпретация регуляризации, совместное правдоподобие данных и модели. Принцип максимума апостериорной вероятности. (1:08:20)\n",
    "8. Гауссовский и лапласовский регуляризаторы. (1:11:50)\n",
    "9. Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Многоклассовая логистическая регрессия. Регуляризованная логистическая регрессия. Калибровка Платта.\n",
    "    * Двухклассовая логистическая регрессия (1:15:10)\n",
    "\n",
    "Замечания:\n",
    "* ***Эмпирическим риском*** называется средняя или суммарная потеря на всех объектах обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод опорных векторов. Параграф №4.5 и №5.6. [Видео](https://www.youtube.com/watch?v=6O4f_sIVffk)\n",
    "1. Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin).\n",
    "    * Задача обучения линейного классификатора (3:00)\n",
    "    * Постановка задачи метода опорных векторов (5:30)\n",
    "    * Оптимальная разделяющая гиперплоскость (8:25)\n",
    "\n",
    "#### Пункты 2,3,4,5 разъясняются на отрезке 14:10 - 23:10\n",
    "2. Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска.\n",
    "3. Кусочно-линейная функция потерь.\n",
    "4. Задача квадратичного программирования и двойственная задача. Понятие опорных векторов.\n",
    "5. Рекомендации по выбору константы C.\n",
    "\n",
    "\n",
    "6. Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера.\n",
    "7. Способы конструктивного построения ядер. Примеры ядер.\n",
    "8. SVM-регрессия.\n",
    "9. Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM.\n",
    "10. Метод релевантных векторов RVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многомерная линейная регрессия + метод главных компонент PCA. Параграф №5. [Видео](https://www.youtube.com/watch?v=tCE_vnPoU44)\n",
    "1. Задача регрессии, многомерная линейная регрессия.\n",
    "2. Метод наименьших квадратов (1:40), его вероятностный смысл (4:05) и геометрический смысл (15:05).\n",
    "    * Многомерная линейная регрессия в матричном виде (9:25)\n",
    "3. Сингулярное разложение. (18:50)\n",
    "4. Проблемы мультиколлинеарности (30:30) и переобучения. \n",
    "\n",
    "#### Необходимо понять как работают $L_1$ и $L_2$ регуляризации\n",
    "5. Регуляризация. Гребневая регрессия (39:10) через сингулярное разложение.\n",
    "6. Методы отбора признаков: Лассо Тибширани (41:00), Elastic Net, сравнение с гребневой регрессией.\n",
    "7. Метод главных компонент и декоррелирующее преобразование Карунена-Лоэва, его связь с сингулярным разложением. (1:02:55)\n",
    "    * Основная теорема метода главных компонент (1:09:00)\n",
    "    \n",
    "    \n",
    "8. Спектральный подход к решению задачи наименьших квадратов.\n",
    "9. Задачи и методы низкоранговых матричных разложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нелинейная регрессия. [Видео](https://www.youtube.com/watch?v=A_jzq0Lpgt0&feature=youtu.be)\n",
    "1. Метод Ньютона-Рафсона (4:15), метод Ньютона-Гаусса.\n",
    "2. Обобщённая аддитивная модель (GAM): метод настройки с возвращениями (backfitting) Хасти-Тибширани.\n",
    "3. Логистическая регрессия. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков. Скоринговые карты и оценивание вероятности дефолта. Риск кредитного портфеля банка.\n",
    "4. Обобщённая линейная модель (GLM) (50:10). Экспоненциальное семейство распределений.\n",
    "    * Обобщенная аддитивная модель (32:05)\n",
    "5. Неквадратичные функции потерь. Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса. (1:19:30)\n",
    "6. Робастная регрессия, функции потерь с горизонтальными асимптотами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критерии выбора моделей и методы отбора признаков. PDF-файл \"Методы оценивания и выбора моделей\" + параграф №4.6 [Видео](https://www.youtube.com/watch?v=uT_H0SFIwbE&feature=youtu.be)\n",
    "1. Критерии качества классификации: чувствительность и специфичность, ROC-кривая и AUC, точность и полнота, AUC-PR. (8:40)\n",
    "    * Алгоритм эффективного построения ROC-кривой (13:25)\n",
    "    * Градиентная максимизация AUC (21:30)\n",
    "    * Алгоритм SG для максимизации AUC (27:45)\n",
    "2. Внутренние и внешние критерии. Эмпирические и аналитические критерии.\n",
    "3. Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости.\n",
    "4. Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Оценка Вапника-Червоненкиса.\n",
    "5. Сложность задачи отбора признаков. Полный перебор.\n",
    "6. Метод добавления и удаления, шаговая регрессия.\n",
    "7. Поиск в глубину, метод ветвей и границ.\n",
    "8. Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА.\n",
    "9. Генетический алгоритм, его сходство с МГУА.\n",
    "10. Случайный поиск и Случайный поиск с адаптацией (СПА).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логические алгоритмы классификации + отдельный PDF с лекцией по ним. [Видео](https://www.youtube.com/watch?v=ntkE5UVSLGw)\n",
    "1. Понятие логической закономерности. (2:35)\n",
    "    * Обучение логических классификаторов (8:25)\n",
    "2. Параметрические семейства закономерностей: конъюнкции пороговых правил, синдромные правила, шары, гиперплоскости. (12:10)\n",
    "3. Переборные алгоритмы синтеза конъюнкций: стохастический локальный поиск, стабилизация, редукция. (21:30)\n",
    "4. Двухкритериальный отбор информативных закономерностей, парето-оптимальный фронт в (p,n)-пространстве. (23:15)\n",
    "5. Статистический критерий информативности, точный тест Фишера. Сравнение областей эвристических и статистических закономерностей. Разнообразие критериев информативности в (p,n)-пространстве. (28:15)\n",
    "    * Различные критерии информативности (38:00)\n",
    "6. Решающее дерево (45:25). Жадная нисходящая стратегия «разделяй и властвуй». Алгоритм ID3 (53:35). Недостатки жадной стратегии и способы их устранения (1:19:35). Проблема переобучения. \n",
    "7. Вывод критериев ветвления (58:47) (1:05:35). Мера нечистоты (impurity) распределения. Энтропийный критерий, критерий Джини (1:07:17).\n",
    "8. Редукция решающих деревьев: предредукция и постредукция (1:22:25). Алгоритм C4.5.\n",
    "9. Деревья регрессии. Алгоритм CART. (1:25:20)\n",
    "10. Небрежные решающие деревья (oblivious decision tree).\n",
    "11. Решающий лес. Случайный лес (Random Forest).\n",
    "12. Решающий пень. Бинаризация признаков. Алгоритм разбиения области значений признака на информативные зоны.\n",
    "13. Решающий список. Жадный алгоритм синтеза списка. Преобразование решающего дерева в решающий список. ***УСТАРЕВШИЙ МЕТОД, ПРАКТИЧЕСКИ НЕ ИСПОЛЬЗУЕТСЯ***\n",
    "\n",
    "Замечания:\n",
    "* Обработка пропущенных значений (1:11:42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные ансамбли + отдельный PDF с лекцией по ним. [Видео](https://www.youtube.com/watch?v=-wa43XNJfVI&feature=youtu.be)\n",
    "1. Основные понятия: базовый алгоритм (0:45), корректирующая операция (3:30) (7:40). \n",
    "2. Простое голосование (комитет большинства) (9:25).\n",
    "    * Проблема разнообразия (diversity) базовых алгоритмов (11:00)\n",
    "3. Стохастические методы (13:33): бэггинг и метод случайных подпространств (15:22).\n",
    "    * Методы стохастического ансамблирования в одном псевдо-коде (16:25 - 18:25)\n",
    "    * Несмещенная оценка ошибок (Out-of-bag) + оценивание важности признаков (18:36)\n",
    "    * Преимущества и ограничения стохастического ансамблирования (30:27)\n",
    "4. Случайный лес (Random Forest) (26:00).\n",
    "5. Взвешенное голосование. Преобразование простого голосования во взвешенное (23:25).\n",
    "----\n",
    "6. Бустинг для задачи классификации с двумя классами (32:55). \n",
    "    * В чём суть бустинга? (36:23)\n",
    "    * Гладкие аппроксимации пороговой фукнции потерь (37:03)\n",
    "7. Алгоритм AdaBoost (38:45). Экспоненциальная аппроксимация пороговой функции потерь (38:45). Процесс последовательного обучения базовых алгоритмов. Теорема о сходимости бустинга (55:50). Идентификация нетипичных объектов (выбросов) (1:02:25).\n",
    "    * Основная теорема бустинга для AdaBoost (43:40)\n",
    "    * Алгоритм AdaBoost в псевдо-коде (57:25)\n",
    "    * Эвристики и рекоммендации для AdaBoost (1:00:45)\n",
    "    * Недостатки AdaBoost и способы их устранения (1:20:00)\n",
    "8. Теоретические обоснования (1:10:25). Обобщающая способность бустинга (1:17:00) и дополнительно (1:07:00).\n",
    "9. Базовые алгоритмы в бустинге. Решающие пни.\n",
    "10. Сравнение бэггинга и бустинга (1:20:00).\n",
    "11. Алгоритм ComBoost. Обобщение на большое число классов. (1:23:45 и до конца)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутые методы ансамблирования [Видео](https://www.youtube.com/watch?v=KRURAkRMo4k) + [можно посмотреть лекцию Евгения Соколова](https://www.youtube.com/watch?v=JkyTxNbiMn4&list=PLEqoHzpnmTfChItexxg2ZfxCsm-8QPsdS&index=11) + Лекция Абакумова про максимальное правдоподобие [Видео](https://www.youtube.com/watch?v=UEJdHH3lbuI&t=2711s) + [Лекция ФПМИ](https://www.youtube.com/watch?v=hTECDpL_JYM&list=LLfF-jFlDAPy6FDif9TJMNKQ&index=154)\n",
    "1. Виды ансамблей. Теоретические обоснования. Анализ смещения и разброса для простого голосования (3:10).\n",
    "    * Разложение ошибки на шум, смещение и разброс (7:25 - 15:30)\n",
    "    * Почему сложные ансамбли не переобучаются? (15:33)\n",
    "2. Градиентный бустинг (20:10 - 30:35). Стохастический градиентный бустинг (40:50).\n",
    "    * Алгоритм градиентного бустинга в псевдо-коде (30:35)\n",
    "3. Варианты бустинга: регрессия, Алгоритм AnyBoost, GentleBoost, LogitBoost, BrownBoost, и другие (46:25).\n",
    "4. Алгоритм XGBoost (47:10). Преимущества XGBoost (56:40)\n",
    "5. Алгоритм CatBoost (58:40). Обработка категориальных признаков (1:20:40).\n",
    "    * Как модифицируется градиентный бустинг (1:14:55)\n",
    "6. Стэкинг. Линейный стэкинг, взвешенный по признакам.\n",
    "    * Блендинг (1:36:05)\n",
    "    * Классический стэкинг (1:39:15)\n",
    "    * Линейный стэкинг, взвешенный по признакам (1:40:25)\n",
    "7. Смесь алгоритмов (квазилинейная композиция), область компетентности, примеры функций компетентности.\n",
    "8. Выпуклые функции потерь. Методы построения смесей: последовательный и иерархический.\n",
    "9. Построение смеси алгоритмов с помощью EM-подобного алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Восстановление плотности распределения. [Видео](https://www.youtube.com/watch?v=ly7v6W9-lB8)\n",
    "1. Параметрическое оценивание плотности. Многомерное нормальное распределение, геометрическая интерпретация.\n",
    "2. Выборочные оценки параметров многомерного нормального распределения. Проблемы мультиколлинеарности и переобучения. Регуляризация ковариационной матрицы.\n",
    "3. Матричное дифференцирование. Вывод оценок параметров многомерного нормального распределения.\n",
    "4. Непараметрическое оценивание плотности. Ядерная оценка плотности Парзена-Розенблатта. Одномерный и многомерный случаи.\n",
    "5. Смесь распределений. EM-алгоритм как метод простых итераций. Обобщённый EM-алгоритм. Стохастический EM-алгоритм.\n",
    "6. Детали реализации EM-алгоритма. Критерий останова. Выбор начального приближения.\n",
    "7. Выбор числа компонентов смеси. Пошаговая стратегия. Иерархический EM-алгоритм.\n",
    "\n",
    "---\n",
    "\n",
    "1. Восстановление плотности - задача обучения без учителя (1:20)\n",
    "    * Порождающая модель - это ... (1:48)\n",
    "    * Правдоподобие выборки - это плотность распределения выборки (3:55)\n",
    "    * Минус логарифм плотности - это функция потерь (4:47)\n",
    "2. Геометрический смысл многомерной нормальной плотности (8:30)\n",
    "3. Проблема мультиколлинеарности (11:53)\n",
    "4. Задача восстановления смеси распределений (18:10)\n",
    "5. EM-алгоритм (Expectation Maximization) и максимизация правдоподобия (22:05)\n",
    "    * EM-алгоритм как способ решения системы уравнений (25:25)\n",
    "    * Вероятностная интерпретация (28:28)\n",
    "    * Доказательство. Условия Каруша-Куна-Таккера (30:20)\n",
    "    * ***EM-алгоритм (35:30)***\n",
    "6. Задача непараметрического восстановления плотности (46:55)\n",
    "    * Локальная непараметрическая оценка Парзена-Розенблатта (52:20)\n",
    "    * Обоснование оценки Парзена-Розенблатта (56:35)\n",
    "    * Два варианта обобщения на многомерный случай (59:05)\n",
    "    * Выбор ширины окна (1:08:10)\n",
    "7. Резюме: три подхода к оцениванию плотностей (1:09:25)\n",
    "8. Задача классификации: минимизация вероятности ошибки (1:13:19)\n",
    "9. Оптимальный байесовский классификатор (1:21:15)\n",
    "10. Наивный байесовский классификатор (1:26:40)\n",
    "    * Наивный байесовский клссификатор в общем виде (1:30:35)\n",
    "    * Приведение распределений к экспоненциальной форме (1:34:34)\n",
    "11. Линейный байесовский клссификатор (1:36:40)\n",
    "12. Метод парзеновского окна (1:43:15)\n",
    "13. Квадратичный дискриминант (1:46:48)\n",
    "14. Линейный дискриминант Фишера (1:50:30)\n",
    "15. Гаусовская смесь с диагональными матрицами ковариации (2:00:15)\n",
    "    * Байесовский классификатор (2:03:30)\n",
    "    * Сеть радиальных базисных функций (RBF - radial basis functions) (2:07:00)\n",
    "16. Резюме по байесовской теории классификации (2:11:18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Восстановление плотности распределения. [Видео](https://www.youtube.com/watch?v=Zfm1rxtjKFc)\n",
    "1. Постановка задачи кластеризации (1:20). Примеры прикладных задач (3:55). Типы кластерных структур (6:15 - 10:10).\n",
    "2. Постановка задачи Semisupervised Learning (частичное обучение) (11:35), примеры приложений (11:35 - 18:20).\n",
    "    * Качество кластеризации в метрическом пространстве (18:30 - 21:10)\n",
    "3. Оптимизационные постановки задач кластеризации и частичного обучения.\n",
    "4. Алгоритм k-средних и ЕМ-алгоритм для разделения гауссовской смеси (22:50 - 32:50).\n",
    "5. Алгоритм DBSCAN. (35:05 - 38:49 - 43:35)\n",
    "6. Агломеративная кластеризация (43:45), Алгоритм Ланса-Вильямса и его частные случаи(43:55 - 51:00).\n",
    "7. Алгоритм построения дендрограммы. Определение числа кластеров. (51:55 - 57:15)\n",
    "8. Свойства сжатия/растяжения и монотонности. (57:18 - 61:00)\n",
    "9. Простые эвристические методы частичного обучения: self-training, co-training, co-learning (1:01:45 - 1:11:40).\n",
    "10. Трансдуктивный метод опорных векторов TSVM (1:11:45 - 1:17:55).\n",
    "11. Алгоритм Expectation-Regularization на основе многоклассовой регуляризированной логистической регрессии (1:18:05 - и до конца)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Лекции Дьяконова по нейронным сетям](https://www.youtube.com/playlist?list=PLaRUeIuewv8BYOrm6HBgJKbGUD-jcBQpW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети: градиентные методы оптимизации. [Видео](https://www.youtube.com/watch?v=Wwv-orQPMDg)\n",
    "1. Биологический нейрон, модель МакКаллока-Питтса как линейный классификатор. Функции активации.\n",
    "2. Проблема полноты. Задача исключающего или. Полнота двухслойных сетей в пространстве булевых функций.\n",
    "3. Алгоритм обратного распространения ошибок. (19:15)\n",
    "4. Быстрые методы стохастического градиента: Поляка, Нестерова, AdaGrad, RMSProp, AdaDelta, Adam, Nadam, диагональный метод Левенберга-Марквардта. (32:50)\n",
    "5. Проблема взрыва градиента и эвристика gradient clipping.\n",
    "6. Метод случайных отключений нейронов (Dropout). Интерпретации Dropout. Обратный Dropout и L2-регуляризация. (42:30)\n",
    "7. Функции активации ReLU и PReLU. Проблема «паралича» сети. (51:50)\n",
    "8. Эвристики для формирования начального приближения. Метод послойной настройки сети.\n",
    "9. Подбор структуры сети: методы постепенного усложнения сети, оптимальное прореживание нейронных сетей (optimal brain damage).\n",
    "\n",
    "***Суперпозиция — композиция функций (сложная функция).***\n",
    "\n",
    "***Полносвязанная нейронная сеть - нейронная сеть, у которой присутствуют все связи между слоями (связи - это стрелочки на картинке).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокие нейронные сети. [Видео](https://www.youtube.com/watch?v=x3TKdZi7Mo4)\n",
    "1. Обоснования глубоких нейронных сетей: выразительные возможности, скорость сходимости при избыточной параметризации.\n",
    "2. Свёрточные нейронные сети (CNN) для изображений. Свёрточный нейрон. Pooling нейрон. Выборка размеченных изображений ImageNet. (18:40)\n",
    "3. ResNet: остаточная нейронная сеть (residual NN). Сквозные связи между слоями (skip connection). (38:55)\n",
    "4. Свёрточные сети для сигналов, текстов, графов, игр. \n",
    "5. Рекуррентные нейронные сети (RNN). Обучение рекуррентных сетей: Backpropagation Through Time (BPTT). (52:00)\n",
    "6. Сети долгой кратковременной памяти (Long short-term memory, LSTM). (1:02:00)\n",
    "7. Рекуррентные сети Gated Recurrent Unit (GRU) и Simple Recurrent Unit (SRU). \n",
    "\n",
    "***Воронцов в видео на 33:50 говорит о статье, в которой приводится теоретическое обоснование того, что лучше увеличивать: число нейронов в сети или число слоёв. Т.е. увеличение какого из этих параметров увеличивает описательную способность нейронной сети. Постараться найти эту статью***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети с обучением без учителя. [Видео](https://www.youtube.com/watch?v=wfbe2yaXAkI)\n",
    "1. Нейронная сеть Кохонена. Конкурентное обучение, стратегии WTA и WTM.\n",
    "2. Самоорганизующаяся карта Кохонена. Применение для визуального анализа данных. Искусство интерпретации карт Кохонена.\n",
    "3. Автокодировщик. Линейный AE, SAE, DAE, CAE, RAE, VAE, AE для классификации, многослойный AE. (29:40)\n",
    "4. Пред-обучение нейронных сетей (pre-training). (1:26:05)\n",
    "5. Перенос обучения (transfer learning). (1:28:30)\n",
    "6. Многозадачное обучение (multi-task learning). (1:32:10)\n",
    "7. Самостоятельное обучение (self-supervised learning). (1:37:50)\n",
    "8. Дистилляция моделей или суррогатное моделирование. (1:42:30)\n",
    "9. Обучение с использованием привилегированной информации (learning using priveleged information, LUPI). (1:47:55)\n",
    "10. Генеративные состязательные сети (generative adversarial net, GAN). (1:53:20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторные представления текстов и графов. [Видео](https://www.youtube.com/watch?v=QJK8PRfKD2g)\n",
    "1. Векторные представления текста. Гипотеза дистрибутивной семантики.\n",
    "    * Синтагматическая и парадигматическая близость слов (3:00)\n",
    "2. Модели CBOW и SGNS из программы word2vec. Иерархический SoftMax. (4:45)\n",
    "    * Модель CBOW (5:45)\n",
    "    * Skip-gram (10:20). ***Узнать, что такое n-gram и skip-gram***\n",
    "    * Иерархический SoftMax (16:25)\n",
    "    * Skip-gram Negative Sampling (SGNS) (25:35)\n",
    "    * Связь word2vec с матричными разложениями (31:55)\n",
    "    * Проверка на задачах семантической близости и аналогии слов (36:15)\n",
    "3. Модель FastText. (42:50)\n",
    "    * Модели векторных представлений для текстов и графов (46:30)\n",
    "4. Векторные представления графов.\n",
    "5. Многомерное шкалирование (multidimensional scaling, MDS). (48:55)\n",
    "6. Векторное представление соседства (stochastic neighbor embedding, SNE и tSNE). (54:35)\n",
    "7. Матричные разложения (graph factorization). (1:09:20)\n",
    "8. Модели случайных блужданий DeepWalk, node2vec. (1:12:30)\n",
    "9. Обобщённый автокодировщик на графах GraphEDM.\n",
    "10. Представление о графовых нейронных сетях (graph neural network, GNN). Передача сообщений по графу (message passing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели внимания и трансформеры. [Видео](https://www.youtube.com/watch?v=KhMweP00S44)\n",
    "1. Задачи обработки и преобразования последовательностей (sequence to sequence). (1:34)\n",
    "    * Рекуррентная сеть для синтеза последовательностей (seq2seq) (5:30)\n",
    "2. Рекуррентная сеть с моделью внимания. (11:30)\n",
    "    * Рекуррентная сеть с вниманием (attention mechanism) (11:30)\n",
    "    * Применение моделей внимания (19:50)\n",
    "    * Разновидности функций сходства векторов (24:30)\n",
    "    * Формула внимания (30:10)\n",
    "3. Разновидности моделей внимания: многомерное, иерархическое, Query–Key–Value, внутреннее (self-attention).\n",
    "    * Многомерное внимание (33:50)\n",
    "    * Иерархическое внимание (38:50)\n",
    "4. Модели внимания на графах (Graph Attention Network). Задача классификации вершин графа. (45:30)\n",
    "5. Трансформеры. Особенности архитектуры кодировщика и декодировщка. (54:20)\n",
    "    * Архитектура трансформера-кодировщика (59:10)\n",
    "    * Несколько дополнений и замечаний к архитектуре (1:08:10)\n",
    "    * Позиционное кодирование (1:12:15)\n",
    "    * Архитектура трансформера декодировщика (1:16:32)\n",
    "6. Критерии обучения и оценивание качества (предобучение). Модель BERT. (1:25:55)\n",
    "    * Модель BERT (1:29:15 - 1:40:05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Тематическое моделирование. [Видео](https://www.youtube.com/watch?v=Eqm8-YqUzAc)\n",
    "### Я не стал смотреть эту лекцию, потому что она про сложные NLP-методы. Для меня это пока довольно сложный материал.\n",
    "1. Задача тематического моделирования коллекции текстовых документов. Метод максимума правдоподобия.\n",
    "2. Лемма о максимизации гладкой функции на симплексах (применение условий Каруша–Куна–Таккера).\n",
    "3. Аддитивная регуляризация тематических моделей. Регуляризованный EM-алгоритм, теорема о стационарной точке. Элементарная интерпретация EM-алгоритма.\n",
    "4. Вероятностный латентный семантический анализ PLSA. ЕМ-алгоритм.\n",
    "5. Латентное размещение Дирихле LDA. Метод максимума апостериорной вероятности. Сглаженная частотная оценка условной вероятности. Небайесовская интерпретация LDA.\n",
    "6. Регуляризаторы разреживания, сглаживания, частичного обучения, декоррелирования.\n",
    "7. Мультимодальная тематическая модель. Мультиязычная тематическая модель.\n",
    "8. Регуляризаторы классификации и регрессии.\n",
    "9. Модель битермов WNTM. Модель связанных документов. Иерархическая тематическая модель.\n",
    "10. Внутренние и внешние критерии качества тематических моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение ранжированию. [Видео](https://www.youtube.com/watch?v=kQ5PeshAO1w)\n",
    "1. Постановка задачи обучения ранжированию. Примеры. (1:00)\n",
    "    * ***Что такое отношение частичного порядка?***\n",
    "    * Примеры задач ранжирования (4:45)\n",
    "2. Поточечные методы Ранговая регрессия. Ранговая классификация, OC-SVM.\n",
    "    * Ранговая регрессия (Ordinal Regression) (7:25)\n",
    "    * Ранговая классификация OC-SVM (Ordinal Classification SVM) (14:45)\n",
    "3. Попарные методы: RankingSVM, RankNet, LambdaRank.\n",
    "    * Попарный подход чаще используется в задачах ранжирования и, как правило, лучше работает.\n",
    "    * Попарный подход (17:35)\n",
    "    * RankingSVM (20:35)\n",
    "    * RankNet (21:35)\n",
    "    * LambdaRank (24:05)\n",
    "4. Списочные методы. (24:05)\n",
    "    * LambdaRank (24:05)\n",
    "5. Признаки в задаче ранжирования поисковой выдачи: текстовые, ссылочные, кликовые. TF-IDF, Okapi BM25, PageRank.\n",
    "    * Задача ранжирования поисковой выдачи (26:55). ***Что такое инвертированный индекс?***\n",
    "    * Кто проставляет оценки релевантности выдачам по запросу (29:20)\n",
    "    * Типы признаков для ранжирования поисковой выдачи (30:27)\n",
    "    * TF-IDF(q,d) - мера релевантности документа d по запросу q (32:52). ***Стоит учесть, что в данном методе происходит подмена вероятностных пространств и с точки зрения математики это не совсем корректно. Но как эвристика это хорошо работает. (35:20)***\n",
    "    * Семейство мер релевантности Best Matching (Okapi BM25) (39:50). Это модификация метода TF-IDF. Данная модификация считается SOTA-решением.\n",
    "    * PageRank - классический ссылочный признак (43:15)\n",
    "6. Критерии качества ранжирования: Precision, MAP, AUC, DCG, NDCG, pFound.\n",
    "    * Оценивание качества поиска (48:15)\n",
    "    * Точность, средняя точность, усреднённая средняя точность (Precision, AP, MAP) (51:00)\n",
    "    * Доля дефектных пар (DP) (56:15)\n",
    "    * DCG - Discounted Cumulative Gain (58:25)\n",
    "    * Яндекс pFound - модель поведения пользователя (1:02:25)\n",
    "    * О ранжировании поисковой выдачи в Яндексе (1:06:25)\n",
    "7. Глубокая структурированная семантическая модель DSSM (Deep Structured Semantic Model).\n",
    "    * Постановка задачи для DSSM (1:09:55). ***Почитать про функцию SoftMax, которая является частью многоклассовой логистической регресии***\n",
    "    * Нейросетевой кодировщик в DSSM (1:18:15)\n",
    "    * Преимущества DSSM (1:24:35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендательные системы. [Видео](https://www.youtube.com/watch?v=FW5UdtMwlpw)\n",
    "1. Задачи коллаборативной фильтрации, транзакционные данные.\n",
    "    * Постановка задачи (0:45)\n",
    "    * Примеры задач (5:30)\n",
    "    * Основные подходы в коллаборативной фильтрации (CF) (13:15)\n",
    "2. Корреляционные методы user-based, item-based. Задача восстановления пропущенных значений. Меры сходства.\n",
    "    * Непараметрическая регрессия для восстановления пропусков (15:50)\n",
    "    * Функции сходства для рейтинговых данных (19:05)\n",
    "    * Функции сходства для бинарных данных (22:10)\n",
    "3. Разреженная линейная модель (Sparse LInear Method, SLIM). (25:40)\n",
    "    * Резюме по Based-методам (32:15)\n",
    "4. Латентные методы на основе матричных разложений. Метод главных компонент для разреженных данных (LFM, Latent Factor Model). Метод стохастического градиента.\n",
    "    * Низкоранговые матричные разложения (matrix factorization) (35:05)\n",
    "    * Сингулярное разложение (singular value devomposition SVD) (40:35)\n",
    "    * Метод чередующихся наименьших квадратов (ALS) (43:40)\n",
    "    * Модель латентных факторов (LFM, Latent Factor Model) (46:06)\n",
    "5. Неотрицательные матричные разложения NNMF. Метод чередующихся наименьших квадратов ALS. Вероятностный латентный семантический анализ PLSA.\n",
    "    * NNMF (Non-Negative Matrix Factorisation) (51:45)\n",
    "    * Мультипликативный алгоритм NNMF (55:45)\n",
    "    * Вероятностный латентный семантический анализ (PLSA) (57:45)\n",
    "6. Модель с учётом неявной информации (implicit feedback). (01:01:05)\n",
    "7. Автокодировщики для коллаборативной фильтрации. (01:07:15)\n",
    "    * Матричное разложение как линейный автокодировщик (01:10:40)\n",
    "    * Нейросетевой автокодировщик для CF (1:14:10)\n",
    "8. Учёт дополнительных признаковых данных в матричных разложениях и автокодировщиках.\n",
    "    * Дополнительные данные в MF и автокодировщиках (01:17:15)\n",
    "9. Линейная и квадратичная регрессионные модели, libFM.\n",
    "    * Дополнительные данные в линейной регрессионной модели (1:20:35)\n",
    "10. Гиперграфовая транзакционная тематическая модель для учёта дополнительных данных (01:24:00).\n",
    "11. Измерение качества рекомендаций. Меры разнообразия (diversity), новизны (novelty), покрытия (coverage), догадливости (serendipity). (01:30:40)\n",
    "    * Оффлайн и онлайн измерения качества рекомендаций (01:33:00)\n",
    "    * Резюме (01:35:10)\n",
    "    * ***Что такое кросс-сейл и апсейл***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск ассоциативных правил. [Видео](https://www.youtube.com/watch?v=jKl2jFQVh94)\n",
    "1. Понятие ассоциативного правила и его связь с понятием логической закономерности.\n",
    "    * Определения и обозначения (1:30). Ключевое определение (4:35)\n",
    "2. Примеры прикладных задач: анализ рыночных корзин, выделение терминов и тематики текстов.\n",
    "    * Классический пример (6:35)\n",
    "    * Ассоциативные правила - это логические закономерности (10:35)\n",
    "3. Алгоритм APriori. Два этапа: поиск частых наборов и рекурсивное порождение ассоциативных правил. Недостатки и пути усовершенствования алгоритма APriori.\n",
    "    * Два этапа построения правил. Свойства антимонотонности (13:20)\n",
    "    * Алгоритм APriori (основная идея - поиск в ширину) (17:05)\n",
    "    * Выделение ассоциативных правил (23:20)\n",
    "    * Модификации алгоритмов индукции ассоциативных правил (33:20)\n",
    "4. Алгоритм FP-growth. Понятия FP-дерева и условного FP-дерева. Два этапа поиска частых наборов в FP-growth: построение FP-дерева и рекурсивное порождение частых наборов.\n",
    "    * Префиксное FP-дерево (FP - frequent pattern). Пример. (37:45) \n",
    "    * Префиксное FP-дерево (FP - frequent pattern) (47:25)\n",
    "    * FP-дерево содержит информацию о всех частых наборах (50:45)\n",
    "    * Этап 1. Алгоритм FP-growth (54:45)\n",
    "    * Этап 2. Рекурсивный поиск частых наборов по FP-дереву (59:20) (1:04:20 - Псевдокод)\n",
    "    * Условное FP-дерево (01:07:30)\n",
    "    * Быстрое построение условного FP-дерева (01:11:25)\n",
    "    * Эффективность алгоритма FP-growth (01:14:25)\n",
    "5. Общее представление о динамических и иерархических методах поиска ассоциативных правил.\n",
    "\n",
    "***Что такое префиксное дерево?***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
