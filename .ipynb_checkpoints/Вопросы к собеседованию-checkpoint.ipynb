{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрическая классификация. Параграф №3 и №5. [Видео](https://www.youtube.com/watch?v=GyOxB2itxnc)\n",
    "1. Гипотезы компактности и непрерывности.\n",
    "2. Обобщённый метрический классификатор.\n",
    "3. Метод ближайших соседей kNN и его обобщения. Подбор числа k по критерию скользящего контроля.\n",
    "4. Метод окна Парзена с постоянной и переменной шириной окна.\n",
    "5. Метод потенциальных функций и его связь с линейной моделью классификации.\n",
    "6. Задача отбора эталонов. Полный скользящий контроль (CCV), формула быстрого вычисления для метода 1NN. Профиль компактности (46:55 в видео).\n",
    "7. Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля.\n",
    "8. Непараметрическая регрессия. Локально взвешенный метод наименьших квадратов. Ядерное сглаживание. (п.5.2)\n",
    "9. Оценка Надарая-Ватсона с постоянной и переменной шириной окна. Выбор функции ядра (1:14:35 в видео) и ширины окна сглаживания. (п.5.2) \n",
    "10. Задача отсева выбросов. Робастная непараметрическая регрессия. Алгоритм LOWESS. (п.5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные методы классификации. Логистическая регрессия. Параграф №4. [Видео](https://www.youtube.com/watch?v=thrPR77K-os&feature=youtu.be)\n",
    "1. Линейный классификатор, модель МакКаллока-Питтса, непрерывные аппроксимации пороговой функции потерь.\n",
    "    * Понятие отступа для разделяющего классификатора (12:10). Обобщение.\n",
    "    * Часто используемые непрервыные функции потерь (16:25)\n",
    "    * Модель МакКаллока-Питтса (18:12)\n",
    "2. Метод стохастического градиента SG. Параграф 4.3\n",
    "    * Градиентный метод численной минимизации (23:05)\n",
    "    * Экспоненциальное скользящее среднее (31:00). Рекуррентная оценка функционала    \n",
    "3. Метод стохастического среднего градиента SAG. (32:20)\n",
    "    * Метод накопления импульса (momentum) (35:00)    \n",
    "4. Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, «выбивание» из локальных минимумов.\n",
    "    * Варианты инициализации весов (37:45)\n",
    "    * Порядок предъявления объектов (42:35)\n",
    "    * Выбор градиентного шага (44:25). Метод Ньютона-Рафсона\n",
    "5. Проблема мультиколлинеарности и переобучения, регуляризация или редукция весов (weight decay). (50:50)\n",
    "    * Повторить что такое линейная зависимость.\n",
    "    * Что такое мультиколлинеарность формально (52:45)\n",
    "    * Регуляризация (сокращение весов) (56:23)\n",
    "6. Вероятностная постановка задачи классификации. Принцип максимума правдоподобия. (1:00:15)\n",
    "7. Вероятностная интерпретация регуляризации, совместное правдоподобие данных и модели. Принцип максимума апостериорной вероятности. (1:08:20)\n",
    "8. Гауссовский и лапласовский регуляризаторы. (1:11:50)\n",
    "9. Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Многоклассовая логистическая регрессия. Регуляризованная логистическая регрессия. Калибровка Платта.\n",
    "    * Двухклассовая логистическая регрессия (1:15:10)\n",
    "\n",
    "Замечания:\n",
    "* ***Эмпирическим риском*** называется средняя или суммарная потеря на всех объектах обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод опорных векторов. Параграф №4.5 и №5.6. [Видео](https://www.youtube.com/watch?v=6O4f_sIVffk)\n",
    "1. Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin).\n",
    "    * Задача обучения линейного классификатора (3:00)\n",
    "    * Постановка задачи метода опорных векторов (5:30)\n",
    "    * Оптимальная разделяющая гиперплоскость (8:25)\n",
    "\n",
    "#### Пункты 2,3,4,5 разъясняются на отрезке 14:10 - 23:10\n",
    "2. Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска.\n",
    "3. Кусочно-линейная функция потерь.\n",
    "4. Задача квадратичного программирования и двойственная задача. Понятие опорных векторов.\n",
    "5. Рекомендации по выбору константы C.\n",
    "\n",
    "\n",
    "6. Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера.\n",
    "7. Способы конструктивного построения ядер. Примеры ядер.\n",
    "8. SVM-регрессия.\n",
    "9. Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM.\n",
    "10. Метод релевантных векторов RVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многомерная линейная регрессия + метод главных компонент PCA. Параграф №5. [Видео](https://www.youtube.com/watch?v=tCE_vnPoU44)\n",
    "1. Задача регрессии, многомерная линейная регрессия.\n",
    "2. Метод наименьших квадратов (1:40), его вероятностный смысл (4:05) и геометрический смысл (15:05).\n",
    "    * Многомерная линейная регрессия в матричном виде (9:25)\n",
    "3. Сингулярное разложение. (18:50)\n",
    "4. Проблемы мультиколлинеарности (30:30) и переобучения. \n",
    "\n",
    "#### Необходимо понять как работают $L_1$ и $L_2$ регуляризации\n",
    "5. Регуляризация. Гребневая регрессия (39:10) через сингулярное разложение.\n",
    "6. Методы отбора признаков: Лассо Тибширани (41:00), Elastic Net, сравнение с гребневой регрессией.\n",
    "7. Метод главных компонент и декоррелирующее преобразование Карунена-Лоэва, его связь с сингулярным разложением. (1:02:55)\n",
    "    * Основная теорема метода главных компонент (1:09:00)\n",
    "    \n",
    "    \n",
    "8. Спектральный подход к решению задачи наименьших квадратов.\n",
    "9. Задачи и методы низкоранговых матричных разложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нелинейная регрессия. [Видео](https://www.youtube.com/watch?v=A_jzq0Lpgt0&feature=youtu.be)\n",
    "1. Метод Ньютона-Рафсона (4:15), метод Ньютона-Гаусса.\n",
    "2. Обобщённая аддитивная модель (GAM): метод настройки с возвращениями (backfitting) Хасти-Тибширани.\n",
    "3. Логистическая регрессия. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков. Скоринговые карты и оценивание вероятности дефолта. Риск кредитного портфеля банка.\n",
    "4. Обобщённая линейная модель (GLM) (50:10). Экспоненциальное семейство распределений.\n",
    "    * Обобщенная аддитивная модель (32:05)\n",
    "5. Неквадратичные функции потерь. Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса. (1:19:30)\n",
    "6. Робастная регрессия, функции потерь с горизонтальными асимптотами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критерии выбора моделей и методы отбора признаков. PDF-файл \"Методы оценивания и выбора моделей\" + параграф №4.6 [Видео](https://www.youtube.com/watch?v=uT_H0SFIwbE&feature=youtu.be)\n",
    "1. Критерии качества классификации: чувствительность и специфичность, ROC-кривая и AUC, точность и полнота, AUC-PR. (8:40)\n",
    "    * Алгоритм эффективного построения ROC-кривой (13:25)\n",
    "    * Градиентная максимизация AUC (21:30)\n",
    "    * Алгоритм SG для максимизации AUC (27:45)\n",
    "2. Внутренние и внешние критерии. Эмпирические и аналитические критерии.\n",
    "3. Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости.\n",
    "4. Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Оценка Вапника-Червоненкиса.\n",
    "5. Сложность задачи отбора признаков. Полный перебор.\n",
    "6. Метод добавления и удаления, шаговая регрессия.\n",
    "7. Поиск в глубину, метод ветвей и границ.\n",
    "8. Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА.\n",
    "9. Генетический алгоритм, его сходство с МГУА.\n",
    "10. Случайный поиск и Случайный поиск с адаптацией (СПА).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логические алгоритмы классификации + отдельный PDF с лекцией по ним. [Видео](https://www.youtube.com/watch?v=ntkE5UVSLGw)\n",
    "1. Понятие логической закономерности. (2:35)\n",
    "    * Обучение логических классификаторов (8:25)\n",
    "2. Параметрические семейства закономерностей: конъюнкции пороговых правил, синдромные правила, шары, гиперплоскости. (12:10)\n",
    "3. Переборные алгоритмы синтеза конъюнкций: стохастический локальный поиск, стабилизация, редукция. (21:30)\n",
    "4. Двухкритериальный отбор информативных закономерностей, парето-оптимальный фронт в (p,n)-пространстве. (23:15)\n",
    "5. Статистический критерий информативности, точный тест Фишера. Сравнение областей эвристических и статистических закономерностей. Разнообразие критериев информативности в (p,n)-пространстве. (28:15)\n",
    "    * Различные критерии информативности (38:00)\n",
    "6. Решающее дерево (45:25). Жадная нисходящая стратегия «разделяй и властвуй». Алгоритм ID3 (53:35). Недостатки жадной стратегии и способы их устранения (1:19:35). Проблема переобучения. \n",
    "7. Вывод критериев ветвления (58:47) (1:05:35). Мера нечистоты (impurity) распределения. Энтропийный критерий, критерий Джини (1:07:17).\n",
    "8. Редукция решающих деревьев: предредукция и постредукция (1:22:25). Алгоритм C4.5.\n",
    "9. Деревья регрессии. Алгоритм CART. (1:25:20)\n",
    "10. Небрежные решающие деревья (oblivious decision tree).\n",
    "11. Решающий лес. Случайный лес (Random Forest).\n",
    "12. Решающий пень. Бинаризация признаков. Алгоритм разбиения области значений признака на информативные зоны.\n",
    "13. Решающий список. Жадный алгоритм синтеза списка. Преобразование решающего дерева в решающий список. ***УСТАРЕВШИЙ МЕТОД, ПРАКТИЧЕСКИ НЕ ИСПОЛЬЗУЕТСЯ***\n",
    "\n",
    "Замечания:\n",
    "* Обработка пропущенных значений (1:11:42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные ансамбли + отдельный PDF с лекцией по ним. [Видео](https://www.youtube.com/watch?v=-wa43XNJfVI&feature=youtu.be)\n",
    "1. Основные понятия: базовый алгоритм (0:45), корректирующая операция (3:30) (7:40). \n",
    "2. Простое голосование (комитет большинства) (9:25).\n",
    "    * Проблема разнообразия (diversity) базовых алгоритмов (11:00)\n",
    "3. Стохастические методы (13:33): бэггинг и метод случайных подпространств (15:22).\n",
    "    * Методы стохастического ансамблирования в одном псевдо-коде (16:25 - 18:25)\n",
    "    * Несмещенная оценка ошибок (Out-of-bag) + оценивание важности признаков (18:36)\n",
    "    * Преимущества и ограничения стохастического ансамблирования (30:27)\n",
    "4. Случайный лес (Random Forest) (26:00).\n",
    "5. Взвешенное голосование. Преобразование простого голосования во взвешенное (23:25).\n",
    "----\n",
    "6. Бустинг для задачи классификации с двумя классами (32:55). \n",
    "    * В чём суть бустинга? (36:23)\n",
    "    * Гладкие аппроксимации пороговой фукнции потерь (37:03)\n",
    "7. Алгоритм AdaBoost (38:45). Экспоненциальная аппроксимация пороговой функции потерь (38:45). Процесс последовательного обучения базовых алгоритмов. Теорема о сходимости бустинга (55:50). Идентификация нетипичных объектов (выбросов) (1:02:25).\n",
    "    * Основная теорема бустинга для AdaBoost (43:40)\n",
    "    * Алгоритм AdaBoost в псевдо-коде (57:25)\n",
    "    * Эвристики и рекоммендации для AdaBoost (1:00:45)\n",
    "    * Недостатки AdaBoost и способы их устранения (1:20:00)\n",
    "8. Теоретические обоснования (1:10:25). Обобщающая способность бустинга (1:17:00) и дополнительно (1:07:00).\n",
    "9. Базовые алгоритмы в бустинге. Решающие пни.\n",
    "10. Сравнение бэггинга и бустинга (1:20:00).\n",
    "11. Алгоритм ComBoost. Обобщение на большое число классов. (1:23:45 и до конца)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутые методы ансамблирования + можно посмотреть лекцию Евгения Соколова. [Видео](https://www.youtube.com/watch?v=KRURAkRMo4k) + Лекция Абакумова про максимальное правдоподобие [Видео](https://www.youtube.com/watch?v=UEJdHH3lbuI&t=2711s)\n",
    "1. Виды ансамблей. Теоретические обоснования. Анализ смещения и разброса для простого голосования (3:10).\n",
    "    * Разложение ошибки на шум, смещение и разброс (7:25 - 15:30)\n",
    "    * Почему сложные ансамбли не переобучаются? (15:33)\n",
    "2. Градиентный бустинг (20:10 - 30:35). Стохастический градиентный бустинг (40:50).\n",
    "    * Алгоритм градиентного бустинга в псевдо-коде (30:35)\n",
    "3. Варианты бустинга: регрессия, Алгоритм AnyBoost, GentleBoost, LogitBoost, BrownBoost, и другие (46:25).\n",
    "4. Алгоритм XGBoost (47:10). Преимущества XGBoost (56:40)\n",
    "5. Алгоритм CatBoost (58:40). Обработка категориальных признаков (1:20:40).\n",
    "    * Как модифицируется градиентный бустинг (1:14:55)\n",
    "6. Стэкинг. Линейный стэкинг, взвешенный по признакам.\n",
    "    * Блендинг (1:36:05)\n",
    "    * Классический стэкинг (1:39:15)\n",
    "    * Линейный стэкинг, взвешенный по признакам (1:40:25)\n",
    "7. Смесь алгоритмов (квазилинейная композиция), область компетентности, примеры функций компетентности.\n",
    "8. Выпуклые функции потерь. Методы построения смесей: последовательный и иерархический.\n",
    "9. Построение смеси алгоритмов с помощью EM-подобного алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Восстановление плотности распределения. [Видео](https://www.youtube.com/watch?v=ly7v6W9-lB8)\n",
    "1. Параметрическое оценивание плотности. Многомерное нормальное распределение, геометрическая интерпретация.\n",
    "2. Выборочные оценки параметров многомерного нормального распределения. Проблемы мультиколлинеарности и переобучения. Регуляризация ковариационной матрицы.\n",
    "3. Матричное дифференцирование. Вывод оценок параметров многомерного нормального распределения.\n",
    "4. Непараметрическое оценивание плотности. Ядерная оценка плотности Парзена-Розенблатта. Одномерный и многомерный случаи.\n",
    "5. Смесь распределений. EM-алгоритм как метод простых итераций. Обобщённый EM-алгоритм. Стохастический EM-алгоритм.\n",
    "6. Детали реализации EM-алгоритма. Критерий останова. Выбор начального приближения.\n",
    "7. Выбор числа компонентов смеси. Пошаговая стратегия. Иерархический EM-алгоритм.\n",
    "\n",
    "---\n",
    "\n",
    "1. Восстановление плотности - задача обучения без учителя (1:20)\n",
    "    * Порождающая модель - это ... (1:48)\n",
    "    * Правдоподобие выборки - это плотность распределения выборки (3:55)\n",
    "    * Минус логарифм плотности - это функция потерь (4:47)\n",
    "2. Геометрический смысл многомерной нормальной плотности (8:30)\n",
    "3. Проблема мультиколлинеарности (11:53)\n",
    "4. Задача восстановления смеси распределений (18:10)\n",
    "5. EM-алгоритм (Expectation Maximization) и максимизация правдоподобия (22:05)\n",
    "    * EM-алгоритм как способ решения системы уравнений (25:25)\n",
    "    * Вероятностная интерпретация (28:28)\n",
    "    * Доказательство. Условия Каруша-Куна-Таккера (30:20)\n",
    "    * ***EM-алгоритм (35:30)***\n",
    "6. Задача непараметрического восстановления плотности (46:55)\n",
    "    * Локальная непараметрическая оценка Парзена-Розенблатта (52:20)\n",
    "    * Обоснование оценки Парзена-Розенблатта (56:35)\n",
    "    * Два варианта обобщения на многомерный случай (59:05)\n",
    "    * Выбор ширины окна (1:08:10)\n",
    "7. Резюме: три подхода к оцениванию плотностей (1:09:25)\n",
    "8. Задача классификации: минимизация вероятности ошибки (1:13:19)\n",
    "9. Оптимальный байесовский классификатор (1:21:15)\n",
    "10. Наивный байесовский классификатор (1:26:40)\n",
    "    * Наивный байесовский клссификатор в общем виде (1:30:35)\n",
    "    * Приведение распределений к экспоненциальной форме (1:34:34)\n",
    "11. Линейный байесовский клссификатор (1:36:40)\n",
    "12. Метод парзеновского окна (1:43:15)\n",
    "13. Квадратичный дискриминант (1:46:48)\n",
    "14. Линейный дискриминант Фишера (1:50:30)\n",
    "15. Гаусовская смесь с диагональными матрицами ковариации (2:00:15)\n",
    "    * Байесовский классификатор (2:03:30)\n",
    "    * Сеть радиальных базисных функций (RBF - radial basis functions) (2:07:00)\n",
    "16. Резюме по байесовской теории классификации (2:11:18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Восстановление плотности распределения. [Видео](https://www.youtube.com/watch?v=Zfm1rxtjKFc)\n",
    "1. Постановка задачи кластеризации (1:20). Примеры прикладных задач (3:55). Типы кластерных структур (6:15 - 10:10).\n",
    "2. Постановка задачи Semisupervised Learning (частичное обучение) (11:35), примеры приложений (11:35 - 18:20).\n",
    "    * Качество кластеризации в метрическом пространстве (18:30 - 21:10)\n",
    "3. Оптимизационные постановки задач кластеризации и частичного обучения.\n",
    "4. Алгоритм k-средних и ЕМ-алгоритм для разделения гауссовской смеси (22:50 - 32:50).\n",
    "5. Алгоритм DBSCAN. (35:05 - 38:49 - 43:35)\n",
    "6. Агломеративная кластеризация (43:45), Алгоритм Ланса-Вильямса и его частные случаи(43:55 - 51:00).\n",
    "7. Алгоритм построения дендрограммы. Определение числа кластеров. (51:55 - 57:15)\n",
    "8. Свойства сжатия/растяжения и монотонности. (57:18 - 61:00)\n",
    "9. Простые эвристические методы частичного обучения: self-training, co-training, co-learning (1:01:45 - 1:11:40).\n",
    "10. Трансдуктивный метод опорных векторов TSVM (1:11:45 - 1:17:55).\n",
    "11. Алгоритм Expectation-Regularization на основе многоклассовой регуляризированной логистической регрессии (1:18:05 - и до конца)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
