{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №1\n",
    "### (Основные понятия и примеры прикладных задач)  [Видео](https://www.youtube.com/watch?v=xccjt6lOoow)\n",
    "1. МНК - метод наименьших квадратов (33:10)\n",
    "2. Из-за чего возникает переобучение (43:20)\n",
    "3. Методологие(pype-line) машинного обучения:\n",
    "    * из лекции (1:11:32)\n",
    "    * Закладка в папке \"Споеседование -> Собеседование по ML -> Жизненный цикл МО (ИТМО)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №2\n",
    "### (Линейный классификатор и стохастический градиент)  [Видео](https://www.youtube.com/watch?v=thrPR77K-os)\n",
    "1. Идея SGD (26:20)\n",
    "2. Идея SAG (34:00)\n",
    "3. Варианты инициализации весов (39:00)\n",
    "4. Выбор градиентного шага (44:30)\n",
    "5. Метод Ньютона-Рафсона (46:15)\n",
    "6. Переобучение лин.моделей/мультиколлинеарность (51:00)\n",
    "7. Регуляризация (56:30)\n",
    "8. Связь ERM и методов вероятностей. Связь МО и вероятностного подхода (1:00:45 смотреть целиком)\n",
    "9. $L_1$ и $L_2$ - регуляризаторы (1:11:52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №4 \n",
    "### (Метрические методы классификации и регрессии)  [Видео](https://www.youtube.com/watch?v=GyOxB2itxnc\")\n",
    "\n",
    "1. Гипотеза непрерывности и компактности (3:00)\n",
    "2. Евклидова метрика, обобщённая метрика Минковского, расстояние Левенштейна(для строк), энергия сжатий и растяжений(для сигналов)\n",
    "3. Обобщенный метрический классификатор. Метрический алгоритм классификации. (12:00)\n",
    "4. Метод k-ближайших соседей (kNN) (14:40)\n",
    "5. Метод окна Парзена (21:40)\n",
    "6. Метод потенциальных функций (29:10)\n",
    "7. Полный скользящий контроль (44:30)\n",
    "8. Резюме по метрическим классификаторам (58:50)\n",
    "9. Непараметрическая регрессия (метод Надарая-Ватсона)\n",
    "10. Выбросы и их отсев (1:23:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №5\n",
    "### (Линейные методы классификации и регрессии: метод опорных векторов)  [Видео](https://www.youtube.com/watch?v=6O4f_sIVffk)\n",
    "1. Операция положительной срезки (6:17 - 6:28) (положительный срез обозначается нижним индексом + или -. Например: $x_+$ или $x_-$\n",
    "2. Регуляризация для линейных моделей (6:25 - 7:20)\n",
    "3. На что влияет $w_0$ в методе опорных векторов (7:30)\n",
    "4. Постановка задачи метода опорных векторов SVM (8:00)\n",
    "5. Классическое альтернативное объяснение задачи SVM (8:20)\n",
    "6. Задача SVM для линейно-неразделимой выборки (15:00)\n",
    "7. Пояснение физического смысла параметров $||w||^2$ и $\\xi$ (переменные типа $\\xi$ еще называют slack variables) в задаче SVM для линейно-неразделимой выборки (16:30)\n",
    "8. Влияение константы $C$ на решение SVM (21:30)\n",
    "9. Условие Каруша-Куна-Таккера (23:45)\n",
    "10. Понятие опорного вектора и типизация объектов (34:00)\n",
    "11. Нелинейное обобщение SVM (48:20). Весь этот кусок я не смотрел ибо очень сложно.\n",
    "12. Классификация с различными ядрами(1:00:35)\n",
    "13. SVM-регрессия (1:13:30)\n",
    "14. LASSO SVM (SVM с $L_1$ регуляризацией) (1:15:30)\n",
    "15. Elastic Net SVM - дважды регулизируемая SVM (1:20:30)\n",
    "16. SFM - support features machine. У данного метода отсуствуют какие-либо недостатки при регуляризации весов. (1:23:40)\n",
    "17. RFM - relevance features machine. Метод релевантных векторов.\n",
    "18. RVM - relevance vector machine - метод релевантных векторов (1:25:00)\n",
    "19. Резюме по SVM (1:29:20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №6\n",
    "### (Многомерная линейная регрессия. Метод главных компонент)  [Видео](https://www.youtube.com/watch?v=tCE_vnPoU44)\n",
    "1. МНК (2:00)\n",
    "2. Связь МНК и ММП(метод максимума правдободобия) (4:20)\n",
    "3. Многомерная линейная регрессия (9:30)\n",
    "    * Узнать получше про ***векторные нормы***. (Норма Фробениуса и т.д.)\n",
    "    * Решение задачи в матричном виде (12:10). ***Посмотреть как дифференцировать по вектору***\n",
    "    * Узнать, что такое ***псевдообратная матрица*** (14:15). Обозначается как $A^+$. На видео поясняет, что такое псевдообратная на (19:28)\n",
    "    * Узнать, что такое ***проекционная матрица*** (14:55)\n",
    "4. Геометрическая интерпретация МНК (15:00)\n",
    "    * Т.е. разные линейные комбинации признаков с параметрами это разные подпространства в пространстве $R^l$ (где $l$ - число объектов). И мы выбираем такое подпространство в котором перпендикуляр, опущенный из $y$ на это подпространство (линейную оболочку векторов) будет минимальным. Т.е. МНК - это опускание перпендикуляра в $R^l$ из $y$ на $L(F)$. (где $L(F)$ - это линейная оболочка столбцов матрицы F)\n",
    "   \n",
    "5. Сингулярное разложение (18:45) (начало объяснения 20:30): \n",
    "    * [Определение собственного вектора](http://mathprofi.ru/sobstvennye_znachenija_i_sobstvennye_vektory.html)\n",
    "    * Узнать что такое собственные значения матрицы\n",
    "    \n",
    "6. Решение МНК через сингулярное разложение (23:00)\n",
    "7. Проблема мультиколлинеарности (30:30) + переобучение (34:20). Тут хорошо это объясняется. Надо понять и заучить.\n",
    "8. Гребневая регрессия или $L_2$-регуляризация (39:10)\n",
    "9. Выбор параметра регуляризации (44:24)\n",
    "10. Регуляризация сокращает \"эффективную размерность\" (47:20)\n",
    "11. LASSO или $L_1$-регуляризация (50:55)\n",
    "12. Сравнение Ridge и LASSO регуляризации (54:45)\n",
    "13. Геометрическая интерпретация отбора признаков (55:30)\n",
    "    * Узнать, что такое линии уровня. \n",
    "14. Другие негладкие регуляризаторы (58:45)\n",
    "15. **Метод главных компонент (сжатие информации или PCA)** (1:02:50):\n",
    "    * Основная теорема PCA (1:09:00)\n",
    "    * Эффективная размерность выборки (1:17:05)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №7\n",
    "### (Нелинейная регрессия)  [Видео](https://www.youtube.com/watch?v=A_jzq0Lpgt0)\n",
    "1. Нелинейная модель регрессии. Метод Ньютона-Рафсона. (4:25)\n",
    "2. Гессиан функции (см. в интернете)\n",
    "3. Метод Ньютона-Гаусса (12:42)\n",
    "4. Логистическая регрессия через метод Ньютона-Рафсона (16:30)\n",
    "5. МНК с итерациаонным взвешиванием объектов (IRLS) (27:30)\n",
    "6. Обобщенная аддитивная модель (Generalized Additive Model) (32:10)\n",
    "    * Что такое ядерное сглаживание и теория сплайнов?\n",
    "7. Метод backfitting (39:25)\n",
    "8. Обобщенная линейная модель (Generalized Linear Model) (45:15)\n",
    "    * Экспоненциальное семейство распределений (50:25)\n",
    "    * Распределение Бернулли нужно для решения задачи бинарной классификации (1:00:40). Т.е. задача одномерной логистической регрессии.\n",
    "    * Биномиальное распределение - многомерная логистическая регрессия.\n",
    "    * Примеры распределений из экспоненциального семейства (1:04:30)\n",
    "    * Максимизация правдоподобия для GLM (1:05:15)\n",
    "***9. Объяснение почему в логистической регрессии используется сигмоидная функция. Пояснение откуда берется log-loss. (1:13:30)***\n",
    "\n",
    "10. Метод наименьших модулей (MAE) (1:19:40)\n",
    "11. Квантильная регрессия (1:21:15)\n",
    "12. Робастная регрессия (1:23:40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №8 + лекция в PDF\n",
    "### (Оценивание качества классификации. Обобщающая способность. Методы отбора признаков.)  [Видео](https://www.youtube.com/watch?v=uT_H0SFIwbE)\n",
    "1. Анализ ошибок классификации (TP, TN, FP, FN) (2:25)\n",
    "2. Функции потерь, зависящие от штрафов за ошибку (4:30)\n",
    "3. Определение ROC-кривой (8:33)\n",
    "4. Алгоритм эффективного построения ROC-кривой (13:27)\n",
    "5. Градиентная максимизация AUC (21:30)\n",
    "6. Дискриминатная функция - что это такое? (28:52)\n",
    "7. Критерий логарифма правдоподобия для задачи оценивания вероятности принадлежности к классу (log-loss) (31:50)\n",
    "8. Точность, полнотоа, AUC-PR (Precision, Recall) (35:30)\n",
    "9. Точность и полнота многоклассовой классификации (40:26)\n",
    "10. Небольшое резюме по оценкам качества классификации (44:33)\n",
    "11. Задачи выбора модели и метода обучения (47:30)\n",
    "    * Оценка качества обучения по прецендентам (51:00). Понятие внутреннего и внешнего критерия оценки качества модели.\n",
    "    * Основное отличие внутренних критериев от внешних. (52:30)\n",
    "    * Кросс-проверка (cross-validation, CV), Leave-one-out, q-fold CV (53:50)\n",
    "    * $t*q$-fold CV - это стандарт \"де факто\" для тестирования методов обучения (56:15)\n",
    "12. Критерии непротиворечивости моделей (58:40)\n",
    "13. Критерии регуляризации (1:00:50)\n",
    "14. Задача отбора признаков (1:02:55). Дискретные алгоритмы оптимизации:\n",
    "    * Алгоритм полного перебора (1:07:00)\n",
    "    * Алгоритм жадного добавления (1:14:00) + Add-Del (1:19:00)\n",
    "    * Усеченный поиск в ширину (beam search) (1:21:41)\n",
    "    * Эволюционный алгоритм (1:29:53)\n",
    "    * Случайный поиск с адаптацией (1:37:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №9 + лекция в PDF\n",
    "### (Логические методы классификации)  [Видео](https://www.youtube.com/watch?v=ntkE5UVSLGw)\n",
    "1. Логическая закономерность (2:50)\n",
    "2. Обучение логических классификаторов (4-х шаговая стратегия) (8:30)\n",
    "    * Шаг №1. Выбор семейства правил для поиска закономерностей (12:00)\n",
    "    * Шаг №2. Выбор алгоритма порождения правил (20:02)\n",
    "    * Шаг №3. Выбор критерия информативности (23:15)\n",
    "    * Шаг №4. Построение классификатора из закономерностей (42:00)\n",
    "3. Решающее дерево (Decision Tree) (45:25)\n",
    "4. Обучение решающего дерева ID3 (Iterative Dichotomiser) (53:35)\n",
    "    * Критерий ветвления (1:05:40)\n",
    "    * Критерий Gini и энтропийный критерий (1:07:15)\n",
    "5. Обработка пропущенных значений (1:11:44)\n",
    "6. Жадная нисходящая стратегия: достоинства и недостатки (1:19:30)\n",
    "7. Усечение дерева: стратегии и post-pruning (1:22:30)\n",
    "8. CART - деревья регресии и классификации (1:25:18)\n",
    "9. Резюме по лекции (1:24:20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №10 + лекция в PDF\n",
    "### (Линейные ансамбли)  [Видео](https://www.youtube.com/watch?v=-wa43XNJfVI)\n",
    "1. Определение ансамбля (0:50)\n",
    "2. Агрегирующие (корректирующие) функции (7:35)\n",
    "3. Проблема разнообразия (diversity) базовых алгоритмов (11:30)\n",
    "4. Методы стохастического ансамблирования (13:40) (17:00)\n",
    "5. Несмещенная оценка ошибок (OOB Out-of-bag) + оценивание важности признаков в OOB (18:40)\n",
    "6. Преобразование простого голосования во взвешенное (23:30). ***Базовые алгоритмы можно трактовать как новые признаки объекта***\n",
    "7. Наивный байесовский классификатор (25:00)\n",
    "8. Случайный лес (Random Forest) (26:02)\n",
    "    * Сайт dyakonov.org очень хорошая площадка с информацией по ML.\n",
    "9. Преимущества и ограничения стохастического ансамблирования (30:26)\n",
    "10. Бустинг для задач классификации с двумя классами (33:10). Две основные эвристики бустинга (36:20). ***Бустинг в отличие от бэггинга строит алгоритмы последовательно друг за другом и каждый следующий алгоритм строится так, чтобы он компенсировал ошибки предыдущего.***\n",
    "11. Гладкие аппроксимации пороговой функции потерь (37:04)\n",
    "12. Адаптивный бустинг (AdaBoost) (38:45). AdaBoost в настоящее время считается устаревшим методом.\n",
    "    * Основная теорема бустинга для AdaBoost (43:40)\n",
    "    * Доказательство теоремы (47:25)\n",
    "    * Следствие 1. (54:40)\n",
    "    * Следствие 2. Сходимость (55:45)\n",
    "    * Алгоритм AdaBoost (57:30)\n",
    "13. Эвристики и рекомендации (1:00:40)\n",
    "14. Удивительный экспериментальный факт об отсутствии переобучения при использовании бутсинга AdaBoost (1:07:05)\n",
    "15. Обоснование бустинга (1:10:30) (1:17:00)\n",
    "16. Недостатки AdaBoost (1:20:00)\n",
    "17. Принцип работы ComBoost (1:24:00). Алгоритм ComBoost (1:27:45)\n",
    "18. Резюме по лекции (1:32:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №11\n",
    "### (Обучение ансамблей)  [Видео](https://www.youtube.com/watch?v=KRURAkRMo4k)\n",
    "1. Анализ смещения-разброса (bias-variance) (4:45)\n",
    "2. ***Разложение ошибки на шум, смещение и разброс (7:25)***\n",
    "3. Анализ смещения и разброса для простого голосования (3:35)\n",
    "4. Почему сложные ансамбли не переобучаются (15:35)\n",
    "5. Градиентный бустинг для произвольной функции потерь (20:20). Посмотреть PDF файл про градиентный бустинг с сайта дьяконова\n",
    "    * Параметрическая аппроксимация градиентного шага (24:33)\n",
    "    * Алгоритм градиентного бустинга (30:38)\n",
    "    * Всякие интересные примеры градиентного бустинга с сайта arogozhnikov.github.io\n",
    "6. Стохастический градиентный бустинг (SGB) (40:55)\n",
    "7. Частные случаи GB: регрессия, AdaBoost и другие (44:10)\n",
    "8. XGBoost (eXtreme GB): популярная и быстрая реализация GB над деревьями (47:10)\n",
    "9. Преимущества XGBoost и другие варианты GB (56:40)\n",
    "10. CatBoost (58:43)\n",
    "    * упорядоченный бустинг (ordered boosting) (1:05:50)\n",
    "    * Алгоритм CatBoost (1:14:55)\n",
    "    * Способы обработки категориальных признаков в CatBoost. Target Statistics (1:20:40)\n",
    "    * Небрежные решающие деревья, которые используются в CatBoost (Oblivious Decision Tree, ODT) (1:31:25)\n",
    "    * Алгоритм обучения ODT (1:34:55)\n",
    "11. Блендинг (Blending) - смешивание базовых алгоритмов. (1:36:07)\n",
    "    * Классический стэкинг (Stacking) (1:39:20)\n",
    "    * Линейный взвешенный стэкинг (1:40:40)\n",
    "    * Квазилинейный ансамбль"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №12\n",
    "### (Восстановление плотности и байесовская теория классификации. Обучение без учителя)  [Видео](https://www.youtube.com/watch?v=ly7v6W9-lB8)\n",
    "1. Восстановление плотности - задача обучения без учителя (1:20)\n",
    "    * Порождающая модель - это ... (1:48)\n",
    "    * Правдоподобие выборки - это плотность распределения выборки (3:55)\n",
    "    * Минус логарифм плотности - это функция потерь (4:47)\n",
    "2. Геометрический смысл многомерной нормальной плотности (8:30)\n",
    "3. Проблема мультиколлинеарности (11:53)\n",
    "4. Задача восстановления смеси распределений (18:10)\n",
    "5. EM-алгоритм (Expectation Maximization) и максимизация правдоподобия (22:05)\n",
    "    * EM-алгоритм как способ решения системы уравнений (25:25)\n",
    "    * Вероятностная интерпретация (28:28)\n",
    "    * Доказательство. Условия Каруша-Куна-Таккера (30:20)\n",
    "    * ***EM-алгоритм (35:30)***\n",
    "6. Задача непараметрического восстановления плотности (46:55)\n",
    "    * Локальная непараметрическая оценка Парзена-Розенблатта (52:20)\n",
    "    * Обоснование оценки Парзена-Розенблатта (56:35)\n",
    "    * Два варианта обобщения на многомерный случай (59:05)\n",
    "    * Выбор ширины окна (1:08:10)\n",
    "7. Резюме: три подхода к оцениванию плотностей (1:09:25)\n",
    "8. Задача классификации: минимизация вероятности ошибки (1:13:19)\n",
    "9. Оптимальный байесовский классификатор (1:21:15)\n",
    "10. Наивный байесовский классификатор (1:26:40)\n",
    "    * Наивный байесовский клссификатор в общем виде (1:30:35)\n",
    "    * Приведение распределений к экспоненциальной форме (1:34:34)\n",
    "11. Линейный байесовский клссификатор (1:36:40)\n",
    "12. Метод парзеновского окна (1:43:15)\n",
    "13. Квадратичный дискриминант (1:46:48)\n",
    "14. Линейный дискриминант Фишера (1:50:30)\n",
    "15. Гаусовская смесь с диагональными матрицами ковариации (2:00:15)\n",
    "    * Байесовский классификатор (2:03:30)\n",
    "    * Сеть радиальных базисных функций (RBF - radial basis functions) (2:07:00)\n",
    "16. Резюме по байесовской теории классификации (2:11:18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
