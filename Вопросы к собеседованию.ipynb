{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрическая классификация. Параграф №3 и №5. [Видео](https://www.youtube.com/watch?v=GyOxB2itxnc)\n",
    "1. Гипотезы компактности и непрерывности.\n",
    "2. Обобщённый метрический классификатор.\n",
    "3. Метод ближайших соседей kNN и его обобщения. Подбор числа k по критерию скользящего контроля.\n",
    "4. Метод окна Парзена с постоянной и переменной шириной окна.\n",
    "5. Метод потенциальных функций и его связь с линейной моделью классификации.\n",
    "6. Задача отбора эталонов. Полный скользящий контроль (CCV), формула быстрого вычисления для метода 1NN. Профиль компактности (46:55 в видео).\n",
    "7. Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля.\n",
    "8. Непараметрическая регрессия. Локально взвешенный метод наименьших квадратов. Ядерное сглаживание. (п.5.2)\n",
    "9. Оценка Надарая-Ватсона с постоянной и переменной шириной окна. Выбор функции ядра (1:14:35 в видео) и ширины окна сглаживания. (п.5.2) \n",
    "10. Задача отсева выбросов. Робастная непараметрическая регрессия. Алгоритм LOWESS. (п.5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные методы классификации. Логистическая регрессия. Параграф №4. [Видео](https://www.youtube.com/watch?v=thrPR77K-os&feature=youtu.be)\n",
    "1. Линейный классификатор, модель МакКаллока-Питтса, непрерывные аппроксимации пороговой функции потерь.\n",
    "    * Понятие отступа для разделяющего классификатора (12:10). Обобщение.\n",
    "    * Часто используемые непрервыные функции потерь (16:25)\n",
    "    * Модель МакКаллока-Питтса (18:12)\n",
    "2. Метод стохастического градиента SG. Параграф 4.3\n",
    "    * Градиентный метод численной минимизации (23:05)\n",
    "    * Экспоненциальное скользящее среднее (31:00). Рекуррентная оценка функционала    \n",
    "3. Метод стохастического среднего градиента SAG. (32:20)\n",
    "    * Метод накопления импульса (momentum) (35:00)    \n",
    "4. Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, «выбивание» из локальных минимумов.\n",
    "    * Варианты инициализации весов (37:45)\n",
    "    * Порядок предъявления объектов (42:35)\n",
    "    * Выбор градиентного шага (44:25). Метод Ньютона-Рафсона\n",
    "5. Проблема мультиколлинеарности и переобучения, регуляризация или редукция весов (weight decay). (50:50)\n",
    "    * Повторить что такое линейная зависимость.\n",
    "    * Что такое мультиколлинеарность формально (52:45)\n",
    "    * Регуляризация (сокращение весов) (56:23)\n",
    "6. Вероятностная постановка задачи классификации. Принцип максимума правдоподобия. (1:00:15)\n",
    "7. Вероятностная интерпретация регуляризации, совместное правдоподобие данных и модели. Принцип максимума апостериорной вероятности. (1:08:20)\n",
    "8. Гауссовский и лапласовский регуляризаторы. (1:11:50)\n",
    "9. Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Многоклассовая логистическая регрессия. Регуляризованная логистическая регрессия. Калибровка Платта.\n",
    "    * Двухклассовая логистическая регрессия (1:15:10)\n",
    "\n",
    "Замечания:\n",
    "* ***Эмпирическим риском*** называется средняя или суммарная потеря на всех объектах обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод опорных векторов. Параграф №4.5 и №5.6. [Видео](https://www.youtube.com/watch?v=6O4f_sIVffk)\n",
    "1. Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin).\n",
    "    * Задача обучения линейного классификатора (3:00)\n",
    "    * Постановка задачи метода опорных векторов (5:30)\n",
    "    * Оптимальная разделяющая гиперплоскость (8:25)\n",
    "\n",
    "#### Пункты 2,3,4,5 разъясняются на отрезке 14:10 - 23:10\n",
    "2. Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска.\n",
    "3. Кусочно-линейная функция потерь.\n",
    "4. Задача квадратичного программирования и двойственная задача. Понятие опорных векторов.\n",
    "5. Рекомендации по выбору константы C.\n",
    "\n",
    "\n",
    "6. Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера.\n",
    "7. Способы конструктивного построения ядер. Примеры ядер.\n",
    "8. SVM-регрессия.\n",
    "9. Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM.\n",
    "10. Метод релевантных векторов RVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многомерная линейная регрессия + метод главных компонент PCA. Параграф №5. [Видео](https://www.youtube.com/watch?v=tCE_vnPoU44)\n",
    "1. Задача регрессии, многомерная линейная регрессия.\n",
    "2. Метод наименьших квадратов (1:40), его вероятностный смысл (4:05) и геометрический смысл (15:05).\n",
    "    * Многомерная линейная регрессия в матричном виде (9:25)\n",
    "3. Сингулярное разложение. (18:50)\n",
    "4. Проблемы мультиколлинеарности (30:30) и переобучения. \n",
    "\n",
    "#### Необходимо понять как работают $L_1$ и $L_2$ регуляризации\n",
    "5. Регуляризация. Гребневая регрессия (39:10) через сингулярное разложение.\n",
    "6. Методы отбора признаков: Лассо Тибширани (41:00), Elastic Net, сравнение с гребневой регрессией.\n",
    "7. Метод главных компонент и декоррелирующее преобразование Карунена-Лоэва, его связь с сингулярным разложением. (1:02:55)\n",
    "    * Основная теорема метода главных компонент (1:09:00)\n",
    "    \n",
    "    \n",
    "8. Спектральный подход к решению задачи наименьших квадратов.\n",
    "9. Задачи и методы низкоранговых матричных разложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нелинейная регрессия. [Видео](https://www.youtube.com/watch?v=A_jzq0Lpgt0&feature=youtu.be)\n",
    "1. Метод Ньютона-Рафсона (4:15), метод Ньютона-Гаусса.\n",
    "2. Обобщённая аддитивная модель (GAM): метод настройки с возвращениями (backfitting) Хасти-Тибширани.\n",
    "3. Логистическая регрессия. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков. Скоринговые карты и оценивание вероятности дефолта. Риск кредитного портфеля банка.\n",
    "4. Обобщённая линейная модель (GLM) (50:10). Экспоненциальное семейство распределений.\n",
    "    * Обобщенная аддитивная модель (32:05)\n",
    "5. Неквадратичные функции потерь. Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса. (1:19:30)\n",
    "6. Робастная регрессия, функции потерь с горизонтальными асимптотами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критерии выбора моделей и методы отбора признаков. PDF-файл \"Методы оценивания и выбора моделей\" + параграф №4.6 [Видео](https://www.youtube.com/watch?v=uT_H0SFIwbE&feature=youtu.be)\n",
    "1. Критерии качества классификации: чувствительность и специфичность, ROC-кривая и AUC, точность и полнота, AUC-PR. (8:40)\n",
    "    * Алгоритм эффективного построения ROC-кривой (13:25)\n",
    "    * Градиентная максимизация AUC (21:30)\n",
    "    * Алгоритм SG для максимизации AUC (27:45)\n",
    "2. Внутренние и внешние критерии. Эмпирические и аналитические критерии.\n",
    "3. Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости.\n",
    "4. Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Оценка Вапника-Червоненкиса.\n",
    "5. Сложность задачи отбора признаков. Полный перебор.\n",
    "6. Метод добавления и удаления, шаговая регрессия.\n",
    "7. Поиск в глубину, метод ветвей и границ.\n",
    "8. Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА.\n",
    "9. Генетический алгоритм, его сходство с МГУА.\n",
    "10. Случайный поиск и Случайный поиск с адаптацией (СПА).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
