{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №1\n",
    "### (Основные понятия и примеры прикладных задач)  [Видео](https://www.youtube.com/watch?v=xccjt6lOoow)\n",
    "1. МНК - метод наименьших квадратов (33:10)\n",
    "2. Из-за чего возникает переобучение (43:20)\n",
    "3. Методологие(pype-line) машинного обучения:\n",
    "    * из лекции (1:11:32)\n",
    "    * Закладка в папке \"Споеседование -> Собеседование по ML -> Жизненный цикл МО (ИТМО)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №2\n",
    "### (Линейный классификатор и стохастический градиент)  [Видео](https://www.youtube.com/watch?v=thrPR77K-os)\n",
    "1. Идея SGD (26:20)\n",
    "2. Идея SAG (34:00)\n",
    "3. Варианты инициализации весов (39:00)\n",
    "4. Выбор градиентного шага (44:30)\n",
    "5. Метод Ньютона-Рафсона (46:15)\n",
    "6. Переобучение лин.моделей/мультиколлинеарность (51:00)\n",
    "7. Регуляризация (56:30)\n",
    "8. Связь ERM и методов вероятностей. Связь МО и вероятностного подхода (1:00:45 смотреть целиком)\n",
    "9. $L_1$ и $L_2$ - регуляризаторы (1:11:52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №4 \n",
    "### (Метрические методы классификации и регрессии)  [Видео](https://www.youtube.com/watch?v=GyOxB2itxnc\")\n",
    "\n",
    "1. Гипотеза непрерывности и компактности (3:00)\n",
    "2. Евклидова метрика, обобщённая метрика Минковского, расстояние Левенштейна(для строк), энергия сжатий и растяжений(для сигналов)\n",
    "3. Обобщенный метрический классификатор. Метрический алгоритм классификации. (12:00)\n",
    "4. Метод k-ближайших соседей (kNN) (14:40)\n",
    "5. Метод окна Парзена (21:40)\n",
    "6. Метод потенциальных функций (29:10)\n",
    "7. Полный скользящий контроль (44:30)\n",
    "8. Резюме по метрическим классификаторам (58:50)\n",
    "9. Непараметрическая регрессия (метод Надарая-Ватсона)\n",
    "10. Выбросы и их отсев (1:23:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №5\n",
    "### (Линейные методы классификации и регрессии: метод опорных векторов)  [Видео](https://www.youtube.com/watch?v=6O4f_sIVffk)\n",
    "1. Операция положительной срезки (6:17 - 6:28) (положительный срез обозначается нижним индексом + или -. Например: $x_+$ или $x_-$\n",
    "2. Регуляризация для линейных моделей (6:25 - 7:20)\n",
    "3. На что влияет $w_0$ в методе опорных векторов (7:30)\n",
    "4. Постановка задачи метода опорных векторов SVM (8:00)\n",
    "5. Классическое альтернативное объяснение задачи SVM (8:20)\n",
    "6. Задача SVM для линейно-неразделимой выборки (15:00)\n",
    "7. Пояснение физического смысла параметров $||w||^2$ и $\\xi$ (переменные типа $\\xi$ еще называют slack variables) в задаче SVM для линейно-неразделимой выборки (16:30)\n",
    "8. Влияение константы $C$ на решение SVM (21:30)\n",
    "9. Условие Каруша-Куна-Таккера (23:45)\n",
    "10. Понятие опорного вектора и типизация объектов (34:00)\n",
    "11. Нелинейное обобщение SVM (48:20). Весь этот кусок я не смотрел ибо очень сложно.\n",
    "12. Классификация с различными ядрами(1:00:35)\n",
    "13. SVM-регрессия (1:13:30)\n",
    "14. LASSO SVM (SVM с $L_1$ регуляризацией) (1:15:30)\n",
    "15. Elastic Net SVM - дважды регулизируемая SVM (1:20:30)\n",
    "16. SFM - support features machine. У данного метода отсуствуют какие-либо недостатки при регуляризации весов. (1:23:40)\n",
    "17. RFM - relevance features machine. Метод релевантных векторов.\n",
    "18. RVM - relevance vector machine - метод релевантных векторов (1:25:00)\n",
    "19. Резюме по SVM (1:29:20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №6\n",
    "### (Многомерная линейная регрессия. Метод главных компонент)  [Видео](https://www.youtube.com/watch?v=tCE_vnPoU44)\n",
    "1. МНК (2:00)\n",
    "2. Связь МНК и ММП(метод максимума правдободобия) (4:20)\n",
    "3. Многомерная линейная регрессия (9:30)\n",
    "    * Узнать получше про ***векторные нормы***. (Норма Фробениуса и т.д.)\n",
    "    * Решение задачи в матричном виде (12:10). ***Посмотреть как дифференцировать по вектору***\n",
    "    * Узнать, что такое ***псевдообратная матрица*** (14:15). Обозначается как $A^+$. На видео поясняет, что такое псевдообратная на (19:28)\n",
    "    * Узнать, что такое ***проекционная матрица*** (14:55)\n",
    "4. Геометрическая интерпретация МНК (15:00)\n",
    "    * Т.е. разные линейные комбинации признаков с параметрами это разные подпространства в пространстве $R^l$ (где $l$ - число объектов). И мы выбираем такое подпространство в котором перпендикуляр, опущенный из $y$ на это подпространство (линейную оболочку векторов) будет минимальным. Т.е. МНК - это опускание перпендикуляра в $R^l$ из $y$ на $L(F)$. (где $L(F)$ - это линейная оболочка столбцов матрицы F)\n",
    "   \n",
    "5. Сингулярное разложение (18:45) (начало объяснения 20:30): \n",
    "    * [Определение собственного вектора](http://mathprofi.ru/sobstvennye_znachenija_i_sobstvennye_vektory.html)\n",
    "    * Узнать что такое собственные значения матрицы\n",
    "    \n",
    "6. Решение МНК через сингулярное разложение (23:00)\n",
    "7. Проблема мультиколлинеарности (30:30) + переобучение (34:20). Тут хорошо это объясняется. Надо понять и заучить.\n",
    "8. Гребневая регрессия или $L_2$-регуляризация (39:10)\n",
    "9. Выбор параметра регуляризации (44:24)\n",
    "10. Регуляризация сокращает \"эффективную размерность\" (47:20)\n",
    "11. LASSO или $L_1$-регуляризация (50:55)\n",
    "12. Сравнение Ridge и LASSO регуляризации (54:45)\n",
    "13. Геометрическая интерпретация отбора признаков (55:30)\n",
    "    * Узнать, что такое линии уровня. \n",
    "14. Другие негладкие регуляризаторы (58:45)\n",
    "15. **Метод главных компонент (сжатие информации или PCA)** (1:02:50):\n",
    "    * Основная теорема PCA (1:09:00)\n",
    "    * Эффективная размерность выборки (1:17:05)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №7\n",
    "### (Нелинейная регрессия)  [Видео](https://www.youtube.com/watch?v=A_jzq0Lpgt0)\n",
    "1. Нелинейная модель регрессии. Метод Ньютона-Рафсона. (4:25)\n",
    "2. Гессиан функции (см. в интернете)\n",
    "3. Метод Ньютона-Гаусса (12:42)\n",
    "4. Логистическая регрессия через метод Ньютона-Рафсона (16:30)\n",
    "5. МНК с итерациаонным взвешиванием объектов (IRLS) (27:30)\n",
    "6. Обобщенная аддитивная модель (Generalized Additive Model) (32:10)\n",
    "    * Что такое ядерное сглаживание и теория сплайнов?\n",
    "7. Метод backfitting (39:25)\n",
    "8. Обобщенная линейная модель (Generalized Linear Model) (45:15)\n",
    "    * Экспоненциальное семейство распределений (50:25)\n",
    "    * Распределение Бернулли нужно для решения задачи бинарной классификации (1:00:40). Т.е. задача одномерной логистической регрессии.\n",
    "    * Биномиальное распределение - многомерная логистическая регрессия.\n",
    "    * Примеры распределений из экспоненциального семейства (1:04:30)\n",
    "    * Максимизация правдоподобия для GLM (1:05:15)\n",
    "***9. Объяснение почему в логистической регрессии используется сигмоидная функция. Пояснение откуда берется log-loss. (1:13:30)***\n",
    "\n",
    "10. Метод наименьших модулей (MAE) (1:19:40)\n",
    "11. Квантильная регрессия (1:21:15)\n",
    "12. Робастная регрессия (1:23:40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №8 + лекция в PDF\n",
    "### (Оценивание качества классификации. Обобщающая способность. Методы отбора признаков.)  [Видео](https://www.youtube.com/watch?v=uT_H0SFIwbE)\n",
    "1. Анализ ошибок классификации (TP, TN, FP, FN) (2:25)\n",
    "2. Функции потерь, зависящие от штрафов за ошибку (4:30)\n",
    "3. Определение ROC-кривой (8:33)\n",
    "4. Алгоритм эффективного построения ROC-кривой (13:27)\n",
    "5. Градиентная максимизация AUC (21:30)\n",
    "6. Дискриминатная функция - что это такое? (28:52)\n",
    "7. Критерий логарифма правдоподобия для задачи оценивания вероятности принадлежности к классу (log-loss) (31:50)\n",
    "8. Точность, полнотоа, AUC-PR (Precision, Recall) (35:30)\n",
    "9. Точность и полнота многоклассовой классификации (40:26)\n",
    "10. Небольшое резюме по оценкам качества классификации (44:33)\n",
    "11. Задачи выбора модели и метода обучения (47:30)\n",
    "    * Оценка качества обучения по прецендентам (51:00). Понятие внутреннего и внешнего критерия оценки качества модели.\n",
    "    * Основное отличие внутренних критериев от внешних. (52:30)\n",
    "    * Кросс-проверка (cross-validation, CV), Leave-one-out, q-fold CV (53:50)\n",
    "    * $t*q$-fold CV - это стандарт \"де факто\" для тестирования методов обучения (56:15)\n",
    "12. Критерии непротиворечивости моделей (58:40)\n",
    "13. Критерии регуляризации (1:00:50)\n",
    "14. Задача отбора признаков (1:02:55). Дискретные алгоритмы оптимизации:\n",
    "    * Алгоритм полного перебора (1:07:00)\n",
    "    * Алгоритм жадного добавления (1:14:00) + Add-Del (1:19:00)\n",
    "    * Усеченный поиск в ширину (beam search) (1:21:41)\n",
    "    * Эволюционный алгоритм (1:29:53)\n",
    "    * Случайный поиск с адаптацией (1:37:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция Воронцова №9 + лекция в PDF\n",
    "### (Логические методы классификации)  [Видео](https://www.youtube.com/watch?v=ntkE5UVSLGw)\n",
    "1. Логическая закономерность (2:50)\n",
    "2. Обучение логических классификаторов (4-х шаговая стратегия) (8:30)\n",
    "    * Шаг №1. Выбор семейства правил для поиска закономерностей (12:00)\n",
    "    * Шаг №2. Выбор алгоритма порождения правил (20:02)\n",
    "    * Шаг №3. Выбор критерия информативности (23:15)\n",
    "    * Шаг №4. Построение классификатора из закономерностей (42:00)\n",
    "3. Решающее дерево (Decision Tree) (45:25)\n",
    "4. Обучение решающего дерева ID3 (Iterative Dichotomiser) (53:35)\n",
    "    * Критерий ветвления (1:05:40)\n",
    "    * Критерий Gini и энтропийный критерий (1:07:15)\n",
    "5. Обработка пропущенных значений (1:11:44)\n",
    "6. Жадная нисходящая стратегия: достоинства и недостатки (1:19:30)\n",
    "7. Усечение дерева: стратегии и post-pruning (1:22:30)\n",
    "8. CART - деревья регресии и классификации (1:25:18)\n",
    "9. Резюме по лекции (1:24:20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
